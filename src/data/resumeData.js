export const RESUME = {
  skills: [
    "Java, Python, C#, R, Scala",
    "SQL, Azure Data Explorer (Kusto), DynamoDB, MySQL, Apache Spark",
    "Pandas, NumPy, Scikit-learn, NLTK, Keras, TensorFlow",
    "Python (Plotly), PowerBI, ggplot2",
    "Amazon Web Services (AWS), Azure",
    "Kubernetes, Airflow, Databricks, AWS, GCP"
  ],
  experience: [
    {
      company: "Amazon Web Services (AWS)",
      role: "Software Development Engineer II",
      period: "2023 — Present",
      location: "San Jose, CA",
      highlights: [
        "Migrated BOM ingestion to AWS-owned data platform, saving ~$20M and streamlining enterprise data operations.",
        "Optimized datacenter mechanical components tracking, delivering $20M+ in cost savings.",
        "Led cross-functional team to ensure BOM accuracy, bridging design and manufacturing."
      ]
    },
    {
      company: "Microsoft",
      role: "Software Development Engineer 2",
      period: "2019 — 2023",
      location: "Redmond, WA",
      highlights: [
        "Migrated internal big data platform to Apache Spark, adhering to GDPR leading to savings of ~$125B in potential fines.",
        "Led cross-functional Azure initiatives optimizing data storage and processing, saving $30M annually.",
        "Mentored interns and new hires, driving adoption of scalable design, code quality standards, and accelerating onboarding."
      ]
    },
    {
      company: "SAP",
      role: "Software Developer",
      period: "2018 — 2019",
      location: "Palo Alto, CA",
      highlights: [
        "Developed a log visualization tool to streamline analysis and improve chatbot interaction interpretation by 20%.",
        "Enhanced bot utterance recognition, reducing false positives and increasing communication accuracy.",
      ]
    },    
    {
      company: "Amazon",
      role: "Software Development Engineer Intern",
      period: "2017",
      location: "Seattle, WA",
      highlights: [
        "Architected and developed a trouble ticket analysis bot, automating workflows and reducing operational workload.",
        "Applied pattern matching and machine learning to automate data extraction, improving team efficiency by 30%.",
        "Led the project end-to-end from conception to production, demonstrating ownership and technical expertise."
      ]
    },
    {
      company: "Conduent",
      role: "Research Scientist Intern",
      period: "2017",
      location: "Webster, NY",
      highlights: [
        "Normalized, cleaned, and documented complex datasets to support reliable modeling and insights.",
        "Designed and implemented an algorithm using operator activity data to accurately measure productivity.",
      ]
    },    {
      company: "Ravi Group of Companies",
      role: "Software Developer",
      period: "2014 — 2015",
      location: "Rochester, NY",
      highlights: [
        "Transitioned inventory system, boosting operational efficiency and data accessibility.",
        "Built a custom CRM tool, streamlining sales processes to increase lead generation and conversions.",
      ]
    },
  ],
  education: [
    {
      school: "Rochester Institute of Technology",
      degree: "M.S. in Computer Science",
      year: "2018"
    },
    {
      school: "University of Mumbai",
      degree: "B.E. in Information Technology",
      year: "2014"
    }
  ],
  publications: [
    {
      title: "Quantifying the Costs of Data Breaches",
      venue: "IFIP AICT, ICCIP 2019 (Springer)",
      url: "https://doi.org/10.1007/978-3-030-34647-8_1"
    },
    {
      title: "Cost Optimization for Data Engineering and API Workloads",
      venue: "Medium",
      url: "https://medium.com/@mananbuddhadev/cost-optimization-for-data-engineering-and-api-workloads-smart-fixes-that-dont-hurt-performance-d80170e9eb4e"
    }
  ]
};
